# ========== Trainer parameters ==========
Trainer:
  experiment_name:                             # Experiment name for logging.
  experiment_directory:                        # Experiment directory for logging.
  write_interval: 100                          # TensorBoard write interval for logging. (timesteps)
  checkpoint_interval:                         # Checkpoint interval for logging. (timesteps)
  wandb: True                                  # If true, log to Weights & Biases.
  wandb_kwargs:                                # Weights & Biases kwargs. https://docs.wandb.ai/ref/python/init
    project: RofuncRL                          # Weights & Biases project name.
    name: ${..experiment_name}                 # Weights & Biases run name.
  rofunc_logger_kwargs:                        # Rofunc BeautyLogger kwargs.
    verbose: True                              # If true, print to stdout.
  maximum_steps: 100000                        # The maximum number of steps to run for.
  random_steps: 0                              # The number of random exploration steps to take.
  start_learning_steps: 0                      # The number of steps to take before starting network updating.
  seed: 42                                     # The random seed.
  rollouts: 16                                 # The number of rollouts before updating.
  max_episode_steps: 200                       # The maximum number of steps per episode.
  eval_flag: False                             # If true, run evaluation.
  eval_freq:  ${multi:${.max_episode_steps},5} # The frequency of evaluation. (timesteps)
  eval_steps: 1000                             # The number of steps to run for evaluation.
  use_eval_thread: False                       # If true, use a separate thread for evaluation.
  inference_steps: 1000                        # The number of steps to run for inference.


# ========== Agent parameters ==========
Agent:
  discount: 0.99                      # The discount factor, gamma.
  td_lambda: 0.95                     # TD(lambda) coefficient (lam) for computing returns and advantages.

  learning_epochs: 8                 # The number of epochs to train for per update.
  mini_batch_size: 8                 # The number of samples per update.

  lr_a: 5e-4                        # Learning rate for actor.
  lr_c: 5e-4                        # Learning rate for critic.
#  lr_scheduler:                    # Learning rate scheduler type.
#  lr_scheduler_kwargs:             # Learning rate scheduler kwargs.
  adam_eps: 1e-5                    # Adam epsilon.

  # If true, use the Generalized Advantage Estimator (GAE)
  # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
  use_gae: True

  entropy_loss_scale: 0.01         # entropy loss scaling factor
  value_loss_scale: 2.0            # value loss scaling factor

  grad_norm_clip: 1.0              # clipping coefficient for the norm of the gradients
  ratio_clip: 0.2                  # clipping coefficient for computing the clipped surrogate objective
  value_clip: 0.2                  # clipping coefficient for computing the value loss (if clip_predicted_values is True)
  clip_predicted_values: True      # clip predicted values during value loss computation

  kl_threshold: 0                  # Initial coefficient for KL divergence.

#  state_preprocessor:            # State preprocessor type.
#  state_preprocessor_kwargs:     # State preprocessor kwargs.
#  value_preprocessor:            # Value preprocessor type.
#  value_preprocessor_kwargs:     # Value preprocessor kwargs.
#  rewards_shaper:                # Rewards shaper type.


# ========= Model parameters ==========
Model:
  use_init: True
  use_same_model: False                 # If true, use the same model for actor and critic.
  use_action_out_tanh: True            # If true, use tanh to scale the action outputs to the action space range.
  use_action_clip: False               # If true, clip actions to the action space range.
  use_action_out_tanh: True            # If true, apply tanh to the output of the actor.
  action_clip: 1.0                     # clipping coefficient for the norm of the actions
  action_scale: 1.0                    # scaling coefficient for the actions
  use_log_std_clip: True               # If true, clip log standard deviations to the range [-20, 2].
  log_std_clip_max: 2.0                # clipping coefficient for the log standard deviations
  log_std_clip_min: -20                # clipping coefficient for the log standard deviations
  fixed_log_std:                        # If empty, use nn.Parameter to learn log_std. Or give a fixed value.

  actor:
    type: "Gaussian"  # ["Beta", "Gaussian"]
    mlp_hidden_dims: [ 512, 256, 128 ]
    mlp_activation: relu
    use_lstm: false
    lstm_cell_size: 256
    lstm_use_prev_action: false
    lstm_use_prev_reward: false
    max_seq_len: 20

  critic:
    mlp_hidden_dims: [ 512, 256, 128 ]
    mlp_activation: relu
    use_lstm: false
    lstm_cell_size: 256
    lstm_use_prev_action: false
    lstm_use_prev_reward: false
    max_seq_len: 20

  state_encoder:  #  Delete this key means no encoder
    encoder_type: "cnn"  # ["resnet", "vit", "cnn"]
    image_size: 224
    use_pretrained: False
    freeze: False
    model_ckpt:
    inp_channels: 3  # only work for 'cnn'
    out_channels: 512

    cnn_args:
      cnn_structure: [ 'conv', 'relu', 'pool', 'conv', 'relu' ]  # TODO: doest not work for now
      cnn_hidden_dims: [ 32, 64, 128 ]
      cnn_kernel_size: 3
      cnn_stride: 1
      cnn_padding: 1
      cnn_dilation: 1
      cnn_activation: 'relu'
      cnn_pooling: max    # ['max', 'avg']
      cnn_pooling_args:
        cnn_pooling_kernel_size: 2
        cnn_pooling_stride: 2
        cnn_pooling_padding: 0
        cnn_pooling_dilation: 1
      mlp_inp_dims: 100352
      mlp_hidden_dims: [ 128 ]
      mlp_activation: 'relu'

    resnet_args:
      sub_type: "resnet18"  # ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']

    vit_args:
      sub_type: "vit_b_16"  # ['vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32']


