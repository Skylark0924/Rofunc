# ========== Trainer parameters ==========
Trainer:
  experiment_name:
  log_directory:
  seed: 0
  num_episodes: 1000
  random_steps: 0
  horizon: 16
  wandb: False
  wandb_kwargs:
  write_interval: 100
  rofunc_logger: False
  rofunc_logger_kwargs: {verbose: True}


# ========== Agent parameters ==========
Agent:
  lr: 0.001  # Learning rate.
  lr_schedule: # Learning rate schedule.

  # If true, use the Generalized Advantage Estimator (GAE)
  # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
  use_gae: True
  lambda: 1.0  # The GAE(lambda) parameter.

  kl_coeff: 0.2 # Initial coefficient for KL divergence.


# ========= Model parameters ==========
model:
  actor:
    mlp_hidden_dims: [ 256, 256 ]
    mlp_activation: tanh
    use_lstm: false
    lstm_cell_size: 256
    lstm_use_prev_action: false
    lstm_use_prev_reward: false
    max_seq_len: 20

  critic:
    mlp_hidden_dims: [ 256, 256 ]
    mlp_activation: tanh
    use_lstm: false
    lstm_cell_size: 256
    lstm_use_prev_action: false
    lstm_use_prev_reward: false
    max_seq_len: 20

