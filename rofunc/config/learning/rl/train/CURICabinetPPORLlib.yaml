# ========== Common Config ==========
num_workers: 0
#num_workers: 1
#num_cpus_per_worker: 20
#num_gpus_per_worker: 1
num_envs_per_worker: 1
num_gpus: 1
framework: tf
seed: 42
#normalize_actions: True
create_env_on_driver: True
horizon: 32

# ========== PPO Config ==========
# Should use a critic as a baseline (otherwise don't use value baseline;
# required for using GAE).
use_critic: True
# If true, use the Generalized Advantage Estimator (GAE)
# with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
use_gae: True
# The GAE(lambda) parameter.
lambda: 1.0
# Initial coefficient for KL divergence.
kl_coeff: 0.2
# Size of batches collected from each worker.
rollout_fragment_length: 1024
# Number of timesteps collected for each SGD round. This defines the size
# of each SGD epoch.
train_batch_size: 4096
# Total SGD batch size across all devices for SGD. This defines the
# minibatch size within each epoch.
sgd_minibatch_size: 4096
# Whether to shuffle sequences in the batch when training (recommended).
shuffle_sequences: True
# Number of SGD iterations in each outer loop (i.e., number of epochs to
# execute per train batch).
num_sgd_iter: 3
# Stepsize of SGD.
lr: 0.001
# Learning rate schedule.
lr_schedule:
# Share layers for value function. If you set this to True, it's important
# to tune vf_loss_coeff.
vf_share_layers: False
# Coefficient of the value function loss. IMPORTANT: you must tune this if
# you set vf_share_layers: True.
vf_loss_coeff: 1.0
# Coefficient of the entropy regularizer.
entropy_coeff: 0.005
# Decay schedule for the entropy regularizer.
entropy_coeff_schedule:
# PPO clip parameter.
clip_param: 0.3
# Clip param for the value function. Note that this is sensitive to the
# scale of the rewards. If your expected V is large, increase this.
vf_clip_param: 10.0
# If specified, clip the global norm of gradients by this amount.
grad_clip: 1.0
# Target value for KL divergence.
kl_target: 0.01
# Whether to rollout complete_episodes or truncate_episodes.
batch_mode: "truncate_episodes"
# Which observation filter to apply to the observation.
observation_filter: "NoFilter"
# Uses the sync samples optimizer instead of the multi-gpu one. This is
# usually slower, but you might want to try it if you run into issues with
# the default optimizer.
simple_optimizer: True
# Whether to fake GPUs (using CPUs).
# Set this to True for debugging on non-GPU machines (set `num_gpus` > 0).
_fake_gpus: False

model: 
    fcnet_hiddens: [256, 256]
    # vf_share_layers: True,
    use_lstm: False
#    lstm_cell_size: 256
#    lstm_use_prev_action: True
#    lstm_use_prev_reward: True
    # use_attention: True