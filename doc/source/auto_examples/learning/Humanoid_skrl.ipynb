{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \"This\" is my example-script\n\nThis example doesn't do much, it just makes a simple plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import isaacgym\n\nimport torch\nimport torch.nn as nn\n\n# Import the skrl components to build the RL system\nfrom skrl.models.torch import Model, GaussianMixin, DeterministicMixin\nfrom skrl.memories.torch import RandomMemory\nfrom skrl.agents.torch.ppo import PPO, PPO_DEFAULT_CONFIG\nfrom skrl.resources.schedulers.torch import KLAdaptiveRL\nfrom skrl.resources.preprocessors.torch import RunningStandardScaler\nfrom skrl.trainers.torch import SequentialTrainer\nfrom skrl.envs.torch import wrap_env\nfrom skrl.envs.torch import load_isaacgym_env_preview4\nfrom skrl.utils import set_seed\n\n# set the seed for reproducibility\nset_seed(42)\n\n\n# Define the shared model (stochastic and deterministic models) for the agent using mixins.\nclass Shared(GaussianMixin, DeterministicMixin, Model):\n    def __init__(self, observation_space, action_space, device, clip_actions=False,\n                 clip_log_std=True, min_log_std=-20, max_log_std=2, reduction=\"sum\"):\n        Model.__init__(self, observation_space, action_space, device)\n        GaussianMixin.__init__(self, clip_actions, clip_log_std, min_log_std, max_log_std, reduction)\n        DeterministicMixin.__init__(self, clip_actions)\n\n        self.net = nn.Sequential(nn.Linear(self.num_observations, 400),\n                                 nn.ELU(),\n                                 nn.Linear(400, 200),\n                                 nn.ELU(),\n                                 nn.Linear(200, 100),\n                                 nn.ELU())\n\n        self.mean_layer = nn.Linear(100, self.num_actions)\n        self.log_std_parameter = nn.Parameter(torch.zeros(self.num_actions))\n\n        self.value_layer = nn.Linear(100, 1)\n\n    def act(self, states, taken_actions, role):\n        if role == \"policy\":\n            return GaussianMixin.act(self, states, taken_actions, role)\n        elif role == \"value\":\n            return DeterministicMixin.act(self, states, taken_actions, role)\n\n    def compute(self, states, taken_actions, role):\n        if role == \"policy\":\n            return self.mean_layer(self.net(states)), self.log_std_parameter\n        elif role == \"value\":\n            return self.value_layer(self.net(states))\n\n\n# Load and wrap the Isaac Gym environment\nenv = load_isaacgym_env_preview4(task_name=\"Humanoid\")  # preview 3 and 4 use the same loader\nenv = wrap_env(env)\n\ndevice = env.device\n\n# Instantiate a RandomMemory as rollout buffer (any memory can be used for this)\nmemory = RandomMemory(memory_size=32, num_envs=env.num_envs, device=device)\n\n# Instantiate the agent's models (function approximators).\n# PPO requires 2 models, visit its documentation for more details\n# https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html#spaces-and-models\nmodels_ppo = {}\nmodels_ppo[\"policy\"] = Shared(env.observation_space, env.action_space, device)\nmodels_ppo[\"value\"] = models_ppo[\"policy\"]  # same instance: shared model\n\n# Configure and instantiate the agent.\n# Only modify some of the default configuration, visit its documentation to see all the options\n# https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html#configuration-and-hyperparameters\ncfg_ppo = PPO_DEFAULT_CONFIG.copy()\ncfg_ppo[\"rollouts\"] = 32  # memory_size\ncfg_ppo[\"learning_epochs\"] = 5\ncfg_ppo[\"mini_batches\"] = 4  # 32 * 4096 / 32768\ncfg_ppo[\"discount_factor\"] = 0.99\ncfg_ppo[\"lambda\"] = 0.95\ncfg_ppo[\"learning_rate\"] = 5e-4\ncfg_ppo[\"learning_rate_scheduler\"] = KLAdaptiveRL\ncfg_ppo[\"learning_rate_scheduler_kwargs\"] = {\"kl_threshold\": 0.008}\ncfg_ppo[\"random_timesteps\"] = 0\ncfg_ppo[\"learning_starts\"] = 0\ncfg_ppo[\"grad_norm_clip\"] = 1.0\ncfg_ppo[\"ratio_clip\"] = 0.2\ncfg_ppo[\"value_clip\"] = 0.2\ncfg_ppo[\"clip_predicted_values\"] = True\ncfg_ppo[\"entropy_loss_scale\"] = 0.0\ncfg_ppo[\"value_loss_scale\"] = 2.0\ncfg_ppo[\"kl_threshold\"] = 0\ncfg_ppo[\"rewards_shaper\"] = lambda rewards, timestep, timesteps: rewards * 0.01\ncfg_ppo[\"state_preprocessor\"] = RunningStandardScaler\ncfg_ppo[\"state_preprocessor_kwargs\"] = {\"size\": env.observation_space, \"device\": device}\ncfg_ppo[\"value_preprocessor\"] = RunningStandardScaler\ncfg_ppo[\"value_preprocessor_kwargs\"] = {\"size\": 1, \"device\": device}\n# logging to TensorBoard and write checkpoints each 160 and 1600 timesteps respectively\ncfg_ppo[\"experiment\"][\"write_interval\"] = 160\ncfg_ppo[\"experiment\"][\"checkpoint_interval\"] = 1600\n\nagent = PPO(models=models_ppo,\n            memory=memory,\n            cfg=cfg_ppo,\n            observation_space=env.observation_space,\n            action_space=env.action_space,\n            device=device)\n\n# Configure and instantiate the RL trainer\ncfg_trainer = {\"timesteps\": 32000, \"headless\": True}\ntrainer = SequentialTrainer(cfg=cfg_trainer, env=env, agents=agent)\n\n# start training\ntrainer.train()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}