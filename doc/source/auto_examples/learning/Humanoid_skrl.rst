
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples\learning\Humanoid_skrl.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_learning_Humanoid_skrl.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_learning_Humanoid_skrl.py:


"This" is my example-script
===========================

This example doesn't do much, it just makes a simple plot

.. GENERATED FROM PYTHON SOURCE LINES 7-120

.. code-block:: default


    import isaacgym

    import torch
    import torch.nn as nn

    # Import the skrl components to build the RL system
    from skrl.models.torch import Model, GaussianMixin, DeterministicMixin
    from skrl.memories.torch import RandomMemory
    from skrl.agents.torch.ppo import PPO, PPO_DEFAULT_CONFIG
    from skrl.resources.schedulers.torch import KLAdaptiveRL
    from skrl.resources.preprocessors.torch import RunningStandardScaler
    from skrl.trainers.torch import SequentialTrainer
    from skrl.envs.torch import wrap_env
    from skrl.envs.torch import load_isaacgym_env_preview4
    from skrl.utils import set_seed

    # set the seed for reproducibility
    set_seed(42)


    # Define the shared model (stochastic and deterministic models) for the agent using mixins.
    class Shared(GaussianMixin, DeterministicMixin, Model):
        def __init__(self, observation_space, action_space, device, clip_actions=False,
                     clip_log_std=True, min_log_std=-20, max_log_std=2, reduction="sum"):
            Model.__init__(self, observation_space, action_space, device)
            GaussianMixin.__init__(self, clip_actions, clip_log_std, min_log_std, max_log_std, reduction)
            DeterministicMixin.__init__(self, clip_actions)

            self.net = nn.Sequential(nn.Linear(self.num_observations, 400),
                                     nn.ELU(),
                                     nn.Linear(400, 200),
                                     nn.ELU(),
                                     nn.Linear(200, 100),
                                     nn.ELU())

            self.mean_layer = nn.Linear(100, self.num_actions)
            self.log_std_parameter = nn.Parameter(torch.zeros(self.num_actions))

            self.value_layer = nn.Linear(100, 1)

        def act(self, states, taken_actions, role):
            if role == "policy":
                return GaussianMixin.act(self, states, taken_actions, role)
            elif role == "value":
                return DeterministicMixin.act(self, states, taken_actions, role)

        def compute(self, states, taken_actions, role):
            if role == "policy":
                return self.mean_layer(self.net(states)), self.log_std_parameter
            elif role == "value":
                return self.value_layer(self.net(states))


    # Load and wrap the Isaac Gym environment
    env = load_isaacgym_env_preview4(task_name="Humanoid")  # preview 3 and 4 use the same loader
    env = wrap_env(env)

    device = env.device

    # Instantiate a RandomMemory as rollout buffer (any memory can be used for this)
    memory = RandomMemory(memory_size=32, num_envs=env.num_envs, device=device)

    # Instantiate the agent's models (function approximators).
    # PPO requires 2 models, visit its documentation for more details
    # https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html#spaces-and-models
    models_ppo = {}
    models_ppo["policy"] = Shared(env.observation_space, env.action_space, device)
    models_ppo["value"] = models_ppo["policy"]  # same instance: shared model

    # Configure and instantiate the agent.
    # Only modify some of the default configuration, visit its documentation to see all the options
    # https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html#configuration-and-hyperparameters
    cfg_ppo = PPO_DEFAULT_CONFIG.copy()
    cfg_ppo["rollouts"] = 32  # memory_size
    cfg_ppo["learning_epochs"] = 5
    cfg_ppo["mini_batches"] = 4  # 32 * 4096 / 32768
    cfg_ppo["discount_factor"] = 0.99
    cfg_ppo["lambda"] = 0.95
    cfg_ppo["learning_rate"] = 5e-4
    cfg_ppo["learning_rate_scheduler"] = KLAdaptiveRL
    cfg_ppo["learning_rate_scheduler_kwargs"] = {"kl_threshold": 0.008}
    cfg_ppo["random_timesteps"] = 0
    cfg_ppo["learning_starts"] = 0
    cfg_ppo["grad_norm_clip"] = 1.0
    cfg_ppo["ratio_clip"] = 0.2
    cfg_ppo["value_clip"] = 0.2
    cfg_ppo["clip_predicted_values"] = True
    cfg_ppo["entropy_loss_scale"] = 0.0
    cfg_ppo["value_loss_scale"] = 2.0
    cfg_ppo["kl_threshold"] = 0
    cfg_ppo["rewards_shaper"] = lambda rewards, timestep, timesteps: rewards * 0.01
    cfg_ppo["state_preprocessor"] = RunningStandardScaler
    cfg_ppo["state_preprocessor_kwargs"] = {"size": env.observation_space, "device": device}
    cfg_ppo["value_preprocessor"] = RunningStandardScaler
    cfg_ppo["value_preprocessor_kwargs"] = {"size": 1, "device": device}
    # logging to TensorBoard and write checkpoints each 160 and 1600 timesteps respectively
    cfg_ppo["experiment"]["write_interval"] = 160
    cfg_ppo["experiment"]["checkpoint_interval"] = 1600

    agent = PPO(models=models_ppo,
                memory=memory,
                cfg=cfg_ppo,
                observation_space=env.observation_space,
                action_space=env.action_space,
                device=device)

    # Configure and instantiate the RL trainer
    cfg_trainer = {"timesteps": 32000, "headless": True}
    trainer = SequentialTrainer(cfg=cfg_trainer, env=env, agents=agent)

    # start training
    trainer.train()


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.000 seconds)


.. _sphx_glr_download_auto_examples_learning_Humanoid_skrl.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: Humanoid_skrl.py <Humanoid_skrl.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: Humanoid_skrl.ipynb <Humanoid_skrl.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
