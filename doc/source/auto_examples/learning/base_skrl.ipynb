{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \"This\" is my example-script\n\nThis example doesn't do much, it just makes a simple plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import datetime\nimport os\n\nimport torch\nimport torch.nn as nn\nfrom skrl.agents.torch.ppo import PPO_DEFAULT_CONFIG\nfrom skrl.models.torch import Model, GaussianMixin, DeterministicMixin\n# Import the skrl components to build the RL system\nfrom skrl.resources.preprocessors.torch import RunningStandardScaler\nfrom skrl.resources.schedulers.torch import KLAdaptiveRL\n\n\n# Define the shared model (stochastic and deterministic models) for the agent using mixins.\nclass Shared(GaussianMixin, DeterministicMixin, Model):\n    def __init__(self, observation_space, action_space, device, clip_actions=False,\n                 clip_log_std=True, min_log_std=-20, max_log_std=2, reduction=\"sum\"):\n        Model.__init__(self, observation_space, action_space, device)\n        GaussianMixin.__init__(self, clip_actions, clip_log_std, min_log_std, max_log_std, reduction)\n        DeterministicMixin.__init__(self, clip_actions)\n\n        self.net = nn.Sequential(nn.Linear(self.num_observations, 256),\n                                 nn.ELU(),\n                                 nn.Linear(256, 128),\n                                 nn.ELU(),\n                                 nn.Linear(128, 64),\n                                 nn.ELU())\n\n        self.mean_layer = nn.Linear(64, self.num_actions)\n        self.log_std_parameter = nn.Parameter(torch.zeros(self.num_actions))\n\n        self.value_layer = nn.Linear(64, 1)\n\n    def act(self, states, taken_actions, role):\n        if role == \"policy\":\n            return GaussianMixin.act(self, states, taken_actions, role)\n        elif role == \"value\":\n            return DeterministicMixin.act(self, states, taken_actions, role)\n\n    def compute(self, states, taken_actions, role):\n        if role == \"policy\":\n            return self.mean_layer(self.net(states)), self.log_std_parameter\n        elif role == \"value\":\n            return self.value_layer(self.net(states))\n\n\ndef set_models_ppo(cfg, env, device):\n    \"\"\"\n    # Instantiate the agent's models (function approximators).\n    # PPO requires 2 models, visit its documentation for more details\n    # https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html#spaces-and-models\n    \"\"\"\n    models_ppo = {}\n    models_ppo[\"policy\"] = Shared(env.observation_space, env.action_space, device)\n    models_ppo[\"value\"] = models_ppo[\"policy\"]  # same instance: shared model\n    return models_ppo\n\n\ndef set_cfg_ppo(cfg, env, device):\n    \"\"\"\n    # Configure and instantiate the agent.\n    # Only modify some default configuration, visit its documentation to see all the options\n    # https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html#configuration-and-hyperparameters\n    \"\"\"\n    cfg_ppo = PPO_DEFAULT_CONFIG.copy()\n    cfg_ppo[\"rollouts\"] = 16  # memory_size\n    cfg_ppo[\"learning_epochs\"] = 8\n    cfg_ppo[\"mini_batches\"] = 8  # 16 * 4096 / 8192\n    cfg_ppo[\"discount_factor\"] = 0.99\n    cfg_ppo[\"lambda\"] = 0.95\n    cfg_ppo[\"learning_rate\"] = 5e-4\n    cfg_ppo[\"learning_rate_scheduler\"] = KLAdaptiveRL\n    cfg_ppo[\"learning_rate_scheduler_kwargs\"] = {\"kl_threshold\": 0.008}\n    cfg_ppo[\"random_timesteps\"] = 0\n    cfg_ppo[\"learning_starts\"] = 0\n    cfg_ppo[\"grad_norm_clip\"] = 1.0\n    cfg_ppo[\"ratio_clip\"] = 0.2\n    cfg_ppo[\"value_clip\"] = 0.2\n    cfg_ppo[\"clip_predicted_values\"] = True\n    cfg_ppo[\"entropy_loss_scale\"] = 0.0\n    cfg_ppo[\"value_loss_scale\"] = 2.0\n    cfg_ppo[\"kl_threshold\"] = 0\n    cfg_ppo[\"rewards_shaper\"] = lambda rewards, timestep, timesteps: rewards * 0.01\n    cfg_ppo[\"state_preprocessor\"] = RunningStandardScaler\n    cfg_ppo[\"state_preprocessor_kwargs\"] = {\"size\": env.observation_space, \"device\": device}\n    cfg_ppo[\"value_preprocessor\"] = RunningStandardScaler\n    cfg_ppo[\"value_preprocessor_kwargs\"] = {\"size\": 1, \"device\": device}\n    # logging to TensorBoard and write checkpoints each 120 and 1200 timesteps respectively\n    cfg_ppo[\"experiment\"][\"write_interval\"] = 120\n    cfg_ppo[\"experiment\"][\"checkpoint_interval\"] = 1200\n    cfg_ppo[\"experiment\"][\"directory\"] = os.path.join(os.getcwd(), \"runs\")\n    cfg_ppo[\"experiment\"][\"experiment_name\"] = \"{}_{}\".format(cfg.train.params.config.name,\n        datetime.datetime.now().strftime(\"%y-%m-%d_%H-%M-%S-%f\"))\n    return cfg_ppo"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}